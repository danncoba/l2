{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "from typing import (\n",
    "    List,\n",
    "    Any,\n",
    "    AsyncGenerator,\n",
    "    Coroutine,\n",
    "    Tuple,\n",
    "    Optional,\n",
    "    TypedDict,\n",
    "    Annotated,\n",
    "    Literal,\n",
    "    Union,\n",
    ")\n",
    "from IPython.display import display, Image\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import AIMessage, SystemMessage, BaseMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.tools import StructuredTool, render_text_description\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from sqlalchemy import Row, RowMapping\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "from agents.consts import (\n",
    "    GUIDANCE_PROMPT,\n",
    "    DISCREPANCY_TEMPLATE,\n",
    "    SUPERVISOR_TEMPLATE,\n",
    "    FEEDBACK_TEMPLATE,\n",
    "    GUIDANCE_TEMPLATE,\n",
    ")\n",
    "from agents.llm_callback import CustomLlmTrackerCallback\n",
    "from db.db import get_session\n",
    "from db.models import Grade, UserSkills, User, Skill\n",
    "from service.service import BaseService\n",
    "from utils.common import convert_agent_msg_to_llm_message"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "\n",
    "search = TavilySearchResults()\n",
    "\n",
    "LITE_LLM_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LITE_LLM_URL = os.getenv(\"OPENAI_BASE_URL\")\n",
    "LITE_MODEL = os.getenv(\"OPENAI_MODEL\")\n",
    "\n",
    "custom_callback = CustomLlmTrackerCallback(\"guidance\")"
   ],
   "id": "e3c0cab83c35bdcd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from agents.dto import AgentMessage, ChatMessage\n",
    "import operator\n",
    "from langgraph.graph import add_messages\n",
    "\n",
    "\n",
    "async def find_current_grade_for_user_and_skill(\n",
    "    user_id: int, skill_id: int\n",
    ") -> UserSkills:\n",
    "    \"\"\"\n",
    "    Utilize to find current expertise and grading level with user id and skill_id\n",
    "    :param user_id: users id\n",
    "    :param skill_id: skill id\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    async for session in get_session():\n",
    "        user_skill_service: BaseService[UserSkills, int, Any, Any] = BaseService(\n",
    "            UserSkills, session\n",
    "        )\n",
    "        filters = {\n",
    "            \"user_id\": user_id,\n",
    "            \"skill_id\": skill_id,\n",
    "        }\n",
    "        user_skill = await user_skill_service.list_all(filters=filters)\n",
    "        if len(user_skill) == 0:\n",
    "            raise Exception(f\"No user_skills found for user_id {user_id}\")\n",
    "        single_user_skill = user_skill[0]\n",
    "        await single_user_skill.awaitable_attrs.user\n",
    "        await single_user_skill.awaitable_attrs.skill\n",
    "        await single_user_skill.awaitable_attrs.grade\n",
    "        return single_user_skill\n",
    "\n",
    "\n",
    "class DiscrepancyValues(BaseModel):\n",
    "    grade_id: int\n",
    "    skill_id: int\n",
    "    user_id: int\n",
    "\n",
    "\n",
    "class GuidanceValue(BaseModel):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "class SupervisorState(TypedDict):\n",
    "    discrepancy: DiscrepancyValues\n",
    "    guidance: GuidanceValue\n",
    "    next_steps: Annotated[List[str], operator.add]\n",
    "    messages: Annotated[List[AgentMessage], operator.add]\n",
    "    chat_messages: Annotated[List[ChatMessage], operator.add]\n",
    "\n",
    "\n",
    "async def discrepancy_agent(state: SupervisorState) -> SupervisorState:\n",
    "    \"\"\"\n",
    "    Discrepancy agent that resolves the discrepancies and explains\n",
    "    the differences between the grades from saved and now provided stated\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    discrepancy_callback = CustomLlmTrackerCallback(\"discrepancy_agent\")\n",
    "    tools = [\n",
    "        StructuredTool.from_function(\n",
    "            function=find_current_grade_for_user_and_skill,\n",
    "            coroutine=find_current_grade_for_user_and_skill,\n",
    "        ),\n",
    "        StructuredTool.from_function(\n",
    "            function=get_grades_or_expertise,\n",
    "            coroutine=get_grades_or_expertise,\n",
    "        ),\n",
    "    ]\n",
    "    model = ChatOpenAI(\n",
    "        temperature=0,\n",
    "        max_tokens=300,\n",
    "        model=LITE_MODEL,\n",
    "        api_key=LITE_LLM_API_KEY,\n",
    "        base_url=LITE_LLM_URL,\n",
    "        streaming=True,\n",
    "        verbose=True,\n",
    "        callbacks=[discrepancy_callback],\n",
    "    )\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_template(DISCREPANCY_TEMPLATE)\n",
    "    prompt = await prompt_template.ainvoke(\n",
    "        input={\n",
    "            \"user_id\": state[\"discrepancy\"].user_id,\n",
    "            \"skill_id\": state[\"discrepancy\"].skill_id,\n",
    "            \"current_grade\": state[\"discrepancy\"].grade_id,\n",
    "        }\n",
    "    )\n",
    "    print(f\"\\n\\nDISCREPANCY AGAIN PROMPT\\n {prompt}\")\n",
    "    agent = create_react_agent(model=model, tools=tools)\n",
    "    response = await agent.ainvoke(prompt)\n",
    "    print(f\"\\n\\nDISCREPANCY AGAIN RESPONSE\\n {response}\")\n",
    "    msg = []\n",
    "    if \"messages\" in response and len(response[\"messages\"]) > 0:\n",
    "        response_msgs = response[\"messages\"][-1]\n",
    "        msg = [\n",
    "            AgentMessage(\n",
    "                message=response_msgs,\n",
    "                role=\"discrepancy\",\n",
    "            )\n",
    "        ]\n",
    "    return {\n",
    "        \"discrepancy\": state[\"discrepancy\"],\n",
    "        \"guidance\": state[\"guidance\"],\n",
    "        \"next_steps\": state[\"next_steps\"],\n",
    "        \"messages\": msg,\n",
    "        \"chat_messages\": state[\"chat_messages\"],\n",
    "    }"
   ],
   "id": "775f0f21259cefd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "from langgraph.types import interrupt\n",
    "\n",
    "\n",
    "async def supervisor_agent(state: SupervisorState) -> SupervisorState:\n",
    "    prompt_template = ChatPromptTemplate.from_template(SUPERVISOR_TEMPLATE)\n",
    "    msgs = []\n",
    "    for msg in state[\"chat_messages\"]:\n",
    "        if msg[\"role\"] == \"human\":\n",
    "            answer = f\"Answer: {msg[\"message\"]}\"\n",
    "            msgs.append(answer)\n",
    "        elif msg[\"role\"] == \"ai\":\n",
    "            question = f\"Question: {msg[\"message\"]}\"\n",
    "            msgs.append(question)\n",
    "    prompt_msgs = \"\\n\".join(msgs)\n",
    "    scratchpad_msgs = [m.message.content for m in state[\"messages\"]]\n",
    "    scratchpad_msgs_str = \"\\n\".join(scratchpad_msgs)\n",
    "    prompt = await prompt_template.ainvoke(\n",
    "        {\"discussion\": prompt_msgs, \"agent_scratchpad\": scratchpad_msgs_str}\n",
    "    )\n",
    "    print(f\"\\n\\nSUPERVISOR AGENT PROMPT\\n {prompt}\")\n",
    "\n",
    "    model = ChatOpenAI(\n",
    "        temperature=0,\n",
    "        max_tokens=300,\n",
    "        model=LITE_MODEL,\n",
    "        api_key=LITE_LLM_API_KEY,\n",
    "        base_url=LITE_LLM_URL,\n",
    "        streaming=True,\n",
    "        verbose=True,\n",
    "        stop=[\"\\nObserve:\"],\n",
    "    )\n",
    "    response = await model.ainvoke(prompt)\n",
    "    print(f\"\\n\\nSUPERVISOR AGENT RESPONSE\\n {response}\")\n",
    "    content = response.content\n",
    "    next_steps = []\n",
    "    match = re.search(r\"\\nCall: (discrepancy|guidance|feedback|grading)\", content)\n",
    "\n",
    "    if match:\n",
    "        print(\"\\n\\n\\nIS MATCHING THIS\\n\\n\\n\")\n",
    "        for val in match.groups():\n",
    "            next_steps.append(val)\n",
    "    else:\n",
    "        next_steps.append(\"finish\")\n",
    "        print(\"\\n\\n\\nFINISHHHHHHH\\n\\n\\n\")\n",
    "\n",
    "    msg = AgentMessage(\n",
    "        message=response,\n",
    "        role=\"supervisor\",\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"discrepancy\": state[\"discrepancy\"],\n",
    "        \"guidance\": state[\"guidance\"],\n",
    "        \"next_steps\": next_steps,\n",
    "        \"messages\": [msg],\n",
    "        \"chat_messages\": state[\"chat_messages\"],\n",
    "    }\n",
    "\n",
    "\n",
    "async def grading_agent(state: SupervisorState) -> SupervisorState:\n",
    "    system_msg = \"\"\"\n",
    "        Based on the provided discussion your job is to confirm the level of expertise of the user!\n",
    "        If you are not sure that the grade or expertise is clearly recognizable please let the the user know\n",
    "        If you're certain state explicitly which expertise is correct for the user!\n",
    "\n",
    "        Discussion:\n",
    "        {discussion}\n",
    "        \"\"\"\n",
    "    prompt_template = ChatPromptTemplate.from_template(system_msg)\n",
    "    msgs = []\n",
    "    for msg in state[\"chat_messages\"]:\n",
    "        if msg[\"role\"] == \"human\":\n",
    "            answer = f\"Answer: {msg[\"message\"]}\"\n",
    "            msgs.append(answer)\n",
    "        elif msg[\"role\"] == \"ai\":\n",
    "            question = f\"Question: {msg[\"message\"]}\"\n",
    "            msgs.append(question)\n",
    "\n",
    "    prompt = await prompt_template.ainvoke(input={\"discussion\": \"\\n\".join(msgs)})\n",
    "    model = ChatOpenAI(\n",
    "        temperature=0,\n",
    "        max_tokens=300,\n",
    "        model=LITE_MODEL,\n",
    "        api_key=LITE_LLM_API_KEY,\n",
    "        base_url=LITE_LLM_URL,\n",
    "        streaming=True,\n",
    "        verbose=True,\n",
    "    )\n",
    "    response = await model.ainvoke(prompt)\n",
    "    print(f\"\\n\\nGRADING AGENT RESPONSE\\n {response}\")\n",
    "    msg = AgentMessage(message=response, role=\"grade\")\n",
    "    return {\n",
    "        \"discrepancy\": state[\"discrepancy\"],\n",
    "        \"guidance\": state[\"guidance\"],\n",
    "        \"next_steps\": state[\"next_steps\"],\n",
    "        \"messages\": [msg],\n",
    "        \"chat_messages\": state[\"chat_messages\"],\n",
    "    }\n",
    "\n",
    "\n",
    "async def evasion_detector_agent(state: SupervisorState) -> SupervisorState:\n",
    "    prompt_template = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        From provided discussion, check whether the user is evading to answer a provided question?\n",
    "\n",
    "        Discussion:\n",
    "        {}\n",
    "\n",
    "        Respond in the following format:\n",
    "        Observe: Your answer\n",
    "        \"\"\"\n",
    "    )\n",
    "    return state\n",
    "\n",
    "\n",
    "async def feedback_agent(state: SupervisorState) -> SupervisorState:\n",
    "    model = ChatOpenAI(\n",
    "        temperature=0,\n",
    "        max_tokens=300,\n",
    "        model=LITE_MODEL,\n",
    "        api_key=LITE_LLM_API_KEY,\n",
    "        base_url=LITE_LLM_URL,\n",
    "        streaming=True,\n",
    "        verbose=True,\n",
    "    )\n",
    "    msgs = convert_agent_msg_to_llm_message(state[\"messages\"])\n",
    "    print(f\"\\n\\nFEEDBACK AGENT PROMPT\\n {msgs}\")\n",
    "    print(f\"\\n\\nFEEDBACK AGENT PROMPT\\n {state['chat_messages'][-1]}\")\n",
    "    prompt_template = ChatPromptTemplate.from_messages(\n",
    "        [SystemMessage(FEEDBACK_TEMPLATE)] + msgs\n",
    "    )\n",
    "    prompt = await prompt_template.ainvoke(input={})\n",
    "    feedback_response = await model.ainvoke(prompt)\n",
    "\n",
    "    interrupt_val = {\n",
    "        \"answer_to_revisit\": \"Please provide additional feedback\",\n",
    "    }\n",
    "    print(f\"\\n\\nFEEDBACK AGENT\\n {interrupt_val}\")\n",
    "    print(f\"\\n\\nFEEDBACK RESPONSE\\n {feedback_response}\")\n",
    "    value = interrupt(\n",
    "        interrupt_val,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"discrepancy\": state[\"discrepancy\"],\n",
    "        \"guidance\": state[\"guidance\"],\n",
    "        \"next_steps\": state[\"next_steps\"],\n",
    "        \"messages\": [interrupt_val],\n",
    "        \"chat_messages\": [feedback_response],\n",
    "    }\n",
    "\n",
    "\n",
    "async def guidance_agent(state: SupervisorState) -> SupervisorState:\n",
    "    print(\"\\n\\n\\nENTERING GUIDANCE\\n\\n\\n\")\n",
    "    tools = [search]\n",
    "    template = ChatPromptTemplate.from_template(GUIDANCE_TEMPLATE)\n",
    "    prompt = await template.ainvoke(\n",
    "        input={\n",
    "            \"tools\": render_text_description(tools),\n",
    "            \"context\": state[\"chat_messages\"][0][\"message\"],\n",
    "            \"answer\": state[\"chat_messages\"][-1][\"message\"],\n",
    "        }\n",
    "    )\n",
    "    model = ChatOpenAI(\n",
    "        temperature=0,\n",
    "        max_tokens=300,\n",
    "        model=LITE_MODEL,\n",
    "        api_key=LITE_LLM_API_KEY,\n",
    "        base_url=LITE_LLM_URL,\n",
    "        streaming=True,\n",
    "        verbose=True,\n",
    "    )\n",
    "    print(f\"\\n\\nGUIDANCE AGENT PROMPT\\n {prompt}\")\n",
    "    agent = create_react_agent(model=model, tools=tools)\n",
    "    agent_response = await agent.ainvoke(prompt)\n",
    "    print(f\"\\n\\nGUIDANCE AGENT RESPONSE\\n {agent_response}\")\n",
    "    msg = AgentMessage(\n",
    "        message=agent_response[\"messages\"][-1],\n",
    "        role=\"guidance\",\n",
    "    )\n",
    "    return {\n",
    "        \"discrepancy\": state[\"discrepancy\"],\n",
    "        \"guidance\": state[\"guidance\"],\n",
    "        \"next_steps\": state[\"next_steps\"],\n",
    "        \"messages\": [msg],\n",
    "        \"chat_messages\": state[\"chat_messages\"],\n",
    "    }\n",
    "\n",
    "\n",
    "async def finish(state: SupervisorState) -> SupervisorState:\n",
    "    return state\n",
    "\n",
    "\n",
    "async def next_step(\n",
    "    state: SupervisorState,\n",
    ") -> Literal[\"guidance\", \"feedback\", \"discrepancy\", \"finish\", \"grading\"]:\n",
    "    if len(state[\"next_steps\"]) > 0:\n",
    "        if state[\"next_steps\"][-1] == \"guidance\":\n",
    "            return \"guidance\"\n",
    "        elif state[\"next_steps\"][-1] == \"discrepancy\":\n",
    "            return \"discrepancy\"\n",
    "        elif state[\"next_steps\"][-1] == \"feedback\":\n",
    "            return \"feedback\"\n",
    "        elif state[\"next_steps\"][-1] == \"grading\":\n",
    "            return \"grading\"\n",
    "    return \"finish\""
   ],
   "id": "75d188fb23c8ab52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class GuidanceHelperStdOutput(BaseModel):\n",
    "    has_user_answered: bool = Field(\n",
    "        description=\"Whether the user has correctly answered the topic at hand\"\n",
    "    )\n",
    "    expertise_level: str = Field(\n",
    "        description=\"The expertise user has self evaluated himself with\"\n",
    "    )\n",
    "    expertise_id: int = Field(description=\"The expertise or grade ID\")\n",
    "    is_more_categories_answered: bool = Field(\n",
    "        description=\"if multiple categories have been selected\", default=False\n",
    "    )\n",
    "    should_admin_be_involved: bool = Field(\n",
    "        description=\"Whether the admin should be involved if user is evading the topic or fooling around\"\n",
    "    )\n",
    "    message: str = Field(description=\"Message to send to the user\")\n",
    "\n",
    "\n",
    "async def get_grades_or_expertise() -> List[Grade]:\n",
    "    \"\"\"\n",
    "    Useful tool to retrieve current grades or expertise level grading system\n",
    "    :return: List of json representing those grades and all their fields\n",
    "    \"\"\"\n",
    "    async for session in get_session():\n",
    "        service: BaseService[Grade, int, Any, Any] = BaseService(Grade, session)\n",
    "        all_db_grades = await service.list_all()\n",
    "        all_grades_json: List[str] = []\n",
    "        for grade in all_db_grades:\n",
    "            json_grade = grade.model_dump_json()\n",
    "            all_grades_json.append(json_grade)\n",
    "        return all_grades_json\n",
    "\n",
    "\n",
    "async def get_current_grade_for_user(\n",
    "    skill_id: int, user_id: int\n",
    ") -> None | Row[Any] | RowMapping | Any:\n",
    "    \"\"\"\n",
    "    Useful tool to expertise level or grade for specific skill for a user\n",
    "    :param skill_id: id of the skill user is looking (ex. Java Development, DevOPS etc...)\n",
    "    :param user_id: id of the user\n",
    "    :return: UserSkill explaining the expertise level for specific user and skill\n",
    "    \"\"\"\n",
    "    async for session in get_session():\n",
    "        service: BaseService[UserSkills, int, Any, Any] = BaseService(\n",
    "            UserSkills, session\n",
    "        )\n",
    "        filters = {\n",
    "            \"skill_id\": skill_id,\n",
    "            \"user_id\": user_id,\n",
    "        }\n",
    "        proper_skill = await service.list_all(filters=filters)\n",
    "        if len(proper_skill) == 0:\n",
    "            return None\n",
    "        return proper_skill[0]\n",
    "\n",
    "\n",
    "async def is_valid_response_for_guidance(\n",
    "    chunk: BaseMessage,\n",
    ") -> Tuple[bool, Optional[AIMessage]]:\n",
    "    if \"agent\" in chunk and \"messages\" in chunk[\"agent\"]:\n",
    "        msg_content = chunk[\"agent\"][\"messages\"][-1]\n",
    "        if isinstance(msg_content, AIMessage) and msg_content.content != \"\":\n",
    "            return True, msg_content\n",
    "    return False, None\n",
    "\n",
    "\n",
    "async def strip_unnecessary_chars(llm_str: AIMessage) -> str:\n",
    "    content = llm_str.content\n",
    "    content = content.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "    return content\n",
    "\n",
    "\n",
    "async def provide_guidance(\n",
    "    msgs: List[str],\n",
    "    user: User,\n",
    "    skill: Skill,\n",
    ") -> AsyncGenerator[GuidanceHelperStdOutput, Any]:\n",
    "    model = ChatOpenAI(\n",
    "        temperature=0,\n",
    "        model=LITE_MODEL,\n",
    "        api_key=LITE_LLM_API_KEY,\n",
    "        base_url=LITE_LLM_URL,\n",
    "        streaming=True,\n",
    "        verbose=True,\n",
    "        callbacks=[custom_callback],\n",
    "    )\n",
    "    tools = [\n",
    "        StructuredTool.from_function(\n",
    "            function=get_grades_or_expertise,\n",
    "            coroutine=get_grades_or_expertise,\n",
    "        ),\n",
    "        StructuredTool.from_function(\n",
    "            function=get_current_grade_for_user,\n",
    "            coroutine=get_current_grade_for_user,\n",
    "        ),\n",
    "    ]\n",
    "    intermediate_steps = []\n",
    "\n",
    "    system_msg = GUIDANCE_PROMPT\n",
    "    agent = create_react_agent(model=model, tools=tools)\n",
    "    async for chunk in agent.astream(\n",
    "        {\n",
    "            \"messages\": [SystemMessage(system_msg)] + msgs,\n",
    "            \"tools\": render_text_description(tools),\n",
    "            \"context\": msgs[0],\n",
    "            \"intermediate_steps\": intermediate_steps,\n",
    "            \"user\": user,\n",
    "            \"skill\": skill,\n",
    "        }\n",
    "    ):\n",
    "        print(\"PROVIDE FEEDBACK\", chunk)\n",
    "        (is_valid, msg_content) = is_valid_response_for_guidance(chunk)\n",
    "        if is_valid:\n",
    "            content = strip_unnecessary_chars(msg_content)\n",
    "            try:\n",
    "                ch = GuidanceHelperStdOutput.model_validate_json(content)\n",
    "                yield ch\n",
    "            except ValidationError:\n",
    "                yield GuidanceHelperStdOutput(\n",
    "                    has_user_answered=False,\n",
    "                    expertise_level=\"\",\n",
    "                    expertise_id=0,\n",
    "                    should_admin_be_involved=False,\n",
    "                    message=content,\n",
    "                )"
   ],
   "id": "7a10fb444068f50b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "state_graph = StateGraph(SupervisorState)\n",
    "\n",
    "state_graph.add_node(\"supervisor\", supervisor_agent)\n",
    "state_graph.add_node(\"discrepancy\", discrepancy_agent)\n",
    "state_graph.add_node(\"guidance\", guidance_agent)\n",
    "state_graph.add_node(\"feedback\", feedback_agent)\n",
    "state_graph.add_node(\"grading\", grading_agent)\n",
    "state_graph.add_node(\"finish\", finish)\n",
    "state_graph.add_edge(START, \"supervisor\")\n",
    "state_graph.add_conditional_edges(\"supervisor\", next_step)\n",
    "state_graph.add_edge(\"discrepancy\", \"supervisor\")\n",
    "state_graph.add_edge(\"guidance\", \"supervisor\")\n",
    "state_graph.add_edge(\"grading\", \"supervisor\")\n",
    "state_graph.add_edge(\"feedback\", \"supervisor\")\n",
    "state_graph.add_edge(\"finish\", END)\n",
    "\n",
    "graph = state_graph.compile()\n",
    "\n",
    "\n",
    "state_vals = SupervisorState(\n",
    "    discrepancy=DiscrepancyValues(skill_id=1, user_id=1, grade_id=7),\n",
    "    guidance=GuidanceValue(messages=[]),\n",
    "    next_steps=[],\n",
    "    messages=[],\n",
    "    chat_messages=[\n",
    "        {\n",
    "          \"message\": \"Expertise Levels in REST API Design\\nWelcome! In this discussion, we will try to create a self assessment on your expertise levels related to the skill of REST API Design. Understanding these levels will help you select the appropriate expertise that aligns with your current knowledge and experience.\\n\\nHere are the available expertise grades:\\n\\nNot Informed: You have no prior knowledge of the topic.\\nInformed Basics: You understand the basic concepts of REST API Design.\\nInformed in Details: You have a deeper understanding of the strategies involved.\\nPractice and Lab Examples: You can apply your knowledge through practical examples and lab work.\\nProduction Maintenance: You are capable of maintaining rest api design systems in a production environment.\\nProduction from Scratch: You can set up rest api design systems from the ground up.\\nEducator/Expert: You possess extensive knowledge and can teach others about REST API Design.\\nConsider your current level of understanding and experience to choose the expertise that best fits you. This will enhance your learning and engagement in our discussion!\",\n",
    "          \"role\": \"ai\"\n",
    "        },\n",
    "        {\n",
    "          \"message\": \"I've spoken at major conferences and am recognized as a thought leader.\",\n",
    "          \"role\": \"human\"\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "async for chunk in graph.astream(state_vals):\n",
    "    print(chunk)"
   ],
   "id": "f56d3d97754f5747",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f5075fe3ec64844e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = await guidance_agent(state_vals)\n",
    "print(response)"
   ],
   "id": "e6eabdc60c7d67db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "display(Image(graph.get_graph().draw_mermaid_png()))",
   "id": "7ba55fac9fbdbc64",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7c3e9dea4521c81a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9acdb0b62d9f523",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
